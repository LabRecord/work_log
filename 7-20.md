# 缓解过拟合

> 总结了一下昨天的实验结果，我突然发现只要有`spectral_flatness`这个特征就会出现过拟合。。。而且过拟合现象还很严重。。。我就尝试了一下权重正则化技术来缓解过拟合（别问我为什么不用Dropout...因为原代码中已经采用了Dropout...）

----------

### **权重正则化技术**

> 奥卡姆剃刀原则：
给出两个解释，最可能正确的解释是更简单的一个 --- 假设较少的解释。 
这个原则也适用于神经网络的模型： 简单的模型比复杂的泛化能力好。


在这种情况下，一个简单的模型指的是模型：
参数值的分布具有较小的熵（或者参数较少）。 因此，减少过拟合的一种常见方法是通过使权重只取小值来限制网络的复杂性，这使得权重值的分布更加规整。
这就是所谓的权重正则化，这是改变网络的损失函数来实现的，在原来的损失函数基础上增加限制权重的成本。
这个成本有两种：

 - L1正则化 - 所增加的成本与权重系数的绝对值成正比（权重的L1范数），权重稀疏化。
 - L2正则化 - 所增加的成本与权重系数（权重的L2范数）的平方成正比。L2正则化在神经网络中也称为权重衰减。 不要让不同的名字混淆你：权重衰减在数学上与L2正则化相 同。

> 在**Keras**中，通过将选择的权重正则化作为关键字参数传递给网络层来增加正则成本。

让我们在电影评论分类网络中添加L2权重正则化。

```python
  
from keras import regularizers
model = models.Sequential()
model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),
activation='relu', input_shape=(10000,)))
model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),
activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))
  
```

$l2（0.001）$意味着该层的权重矩阵中的每个系数将增加$0.001 * $**weight_coefficient_value**到网络的总损失。 
请注意，由于这种惩罚只是在训练时间增加的，这个网络的损失在训练时比在测试时间要高得多。

下图显示了$l2$权重正则化对验证集损失的影响。
正如你所看到的那样，即使两个模型具有相同数量的参数，具有$l2$正则化的模型（点）也比参考模型（十字）更能减少过拟合。

![Figure 1](https://github.com/THU-iar-AiLab/experiment_results/raw/master/images/1.png)

除了L2正则化，你还可以使用以下Keras权重正则方法。

```python
  
from keras import regularizers
regularizers.l1(0.001) # L1正则化
regularizers.l1_l2(l1=0.001, l2=0.001) # 同时进行L1和L2正则化
  
```

----------

> [参考链接](https://zhuanlan.zhihu.com/p/33138871)

----------

### **实验结果**

与![Figure 2](https://github.com/THU-iar-AiLab/experiment_results/raw/master/images/2.png)
的对比：

![Figure 3](https://github.com/THU-iar-AiLab/experiment_results/raw/master/images/3.png)

emmm...好像并没有啥效果。。。
再试试其他办法吧。。。

----------

PS：今天在FIT楼突然发现两个电梯其中有一个没有6楼的按钮，问为什么，说6楼是叉院，emmm....

----------
