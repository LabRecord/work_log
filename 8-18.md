# Audio modality

> *Similar to text and visual feature analysis, emotion and sentiment analysis through audio features has specific components. Several prosodic and acoustic features have been used in the literature to teach machines how to detect emotions [95–98]. Since emotional characteristics are more prominent in prosodic features, these features are widely used in the literature [99,100].*

与文本和视觉特征分析类似，通过音频特征进行的情绪和情感分析具有特定的组成部分。在文献中已经使用了几种韵律和声学特征来教授机器如何检测情绪[95-98]。由于情绪特征在韵律特征中更为突出，因此这些特征在文献中被广泛使用[99,100]。

> *Researchers started targeting affective reactions to everyday sounds [101], which have ultimately led to enormous applications to date, both in unimodal and multimodal analysis. The current trend is to understand affect in naturalistic videos [102–104], e.g., spontaneous dialogs, audio recordings collected in call centers, interviews, etc. Early research on extraction of audio features focused on the phonetic and acoustic properties of spoken language. With the help of psychological studies related to emotion, it was found that vocal parameters, especially pitch, intensity, speaking rate and voice quality play an important role in recognition of emotion and sentiment analysis [98].*

研究人员开始针对日常声音进行情感反应[101]，这最终导致了迄今为止在单模态和多模态分析中的巨大应用。目前的趋势是理解在自然的视频[102-104]中的影响，例如，自发的对话，在呼叫中心收集的录音，采访等。早期的音频特征提取研究主要集中在口语的语音和声学特性上。在与情绪相关的心理学研究的帮助下，发现声音参数，特别是音高，强度，说话率和语音质量在情绪和情感分析的识别中起着重要作用[98]。

> *Further studies showed that acoustic parameters change not only through oral variations, but are also dependent on personality traits. Various works have been carried out based on the types of features that are needed for better analysis [105,106]. Researchers have found pitch and energy related features playing a key role in affect recognition. Other features that have been used by some researchers for feature extraction include formants, mel frequency cepstral coefficients (MFCC), pause, teager energy operated based features, log frequency power coefficients (LFPC) and linear predic- tion cepstral coefficients (LPCC).*

进一步的研究表明，声学参数不仅通过口腔变化而改变，而且还取决于人格特质。为了更好的分析基于所需的特征类型已经进行了各种工作[105,106]。研究人员发现，音高和能量相关的特征在情感识别中起着关键作用。一些研究人员用于特征提取的其他特征包括共振峰，梅尔频率倒谱系数（MFCC），暂停，基于能量操作的特征，对数频率功率系数（LFPC）和线性预测倒谱系数（LPCC）。

-----
Some of the important audio features are described briefly below:

• Mel Frequency Cepstral Coefficients (MFCC) are coefficients that collectively form a mel-frequency cepstrum (MFC). The MFC is a short-term power spectrum of a sound or an audio clip, which approximates the human auditory system more closely than any other available linearly-spaced frequency band distribution. This feature is calculated based on the linear cosine transform of a log power spectrum, on a mel-frequency scaling.

• Spectral centroid indicates the center of mass of the magnitude spectrum, which simply provides an indication of the bright- ness of a sound.

• Spectral flux defines how quickly the power spectrum of a sig- nal is changing. This feature is usually calculated by taking the Euclidean distance between two normalized spectra.

• Beat Histogram is a histogram showing the strength of different rhythmic periodicities in a signal. It is typically calculated by taking the RMS of 256 windows and then taking the FFT of the output.

• Beat sum is used to find regular beats in a signal. It is calculated as the sum of all entries in the beat histogram.

• Strongest beat is the strongest beat in a signal and is found by identifying the strongest bin the beat histogram.

• Pause duration is the time the speaker is silent in an audio seg- ment.

• Pitch is the quality of a sound governed by the rate of vibrations producing it; the degree of highness or lowness of a tone.

• The Perceptual Linear Predictive Coefficients (PLP) are created from linear prediction coefficients (LPC) by performing percep- tual processing before autoregressive modeling and followed by cepstral conversion.

一些重要的音频特征简要描述如下：

•梅尔频率倒谱系数（MFCC）是共同形成梅尔频率倒谱（MFC）的系数。 MFC是声音或音频片段的短期功率谱，其比任何其他可用的线性间隔频带分布更接近人类听觉系统。该特征基于对数功率谱的线性余弦变换，在梅尔频率缩放上计算。

•光谱质心表示幅度谱的质心，它只是表示声音的亮度。

•光谱通量定义信号功率谱的变化速度。该特征通常通过取两个归一化光谱之间的欧几里德距离来计算。

•节拍直方图是一个直方图，显示信号中不同节奏周期的强度。通常通过取256个窗口的RMS然后取输出的FFT来计算。

•节拍总和用于查找信号中的常规节拍。它被计算为节拍直方图中所有条目的总和。

•最强节拍是信号中最强的节拍，通过识别节拍直方图的最强音箱来找到。

•暂停持续时间是扬声器在音频段中静音的时间。

•音高是由产生振动的速率决定的声音质量;音调的高低程度。

•感知线性预测系数（PLP）是根据线性预测系数（LPC）通过在自回归建模之前执行感知处理并随后进行倒谱转换而创建的。

-----
> *OpenSMILE [107] is a popular audio feature extraction toolkit which is able to extract all the key features as elaborated above. This framework is shown in Fig. 6. The affective reactions to sound have been classified as discrete feeling states and states based on dimensions [101,108]. Discrete feeling states are defined as emotions that are spontaneous, uncontrollable or, in other words, universal emotions. The states based on dimension are hedonic valence (pleasantness), arousal (activation, intensity) and dominance. Recent studies on speech-based emotion analysis [98,109–111] have focused on identifying several acoustic features such as fundamental frequency (pitch), intensity of utterance [112], bandwidth, and duration. The speaker-dependent approach often gives much better results than the speaker-independent approach, as shown by benchmark results of Navas et al. [113], where about 98% accuracy was achieved using the Gaussian mixture model (GMM) as a classifier, with prosodic, voice quality as well as MFCCs employed as speech features. However, the speaker-dependent approach is not feasible in many practical applications that deal with a very large number of users.*

OpenSMILE [107]是一种流行的音频特征提取工具包，它能够提取上面详述的所有关键特性。该框架如图6所示。对声音的情感反应被分类为基于尺寸的离散感觉状态和状态[101,108]。离散的感觉状态被定义为自发的，无法控制的，或者换句话说，普遍的情绪。基于维度的状态是享乐效价（愉悦），唤醒（激活，强度）和支配地位。最近对基于语音的情绪分析[98,109-111]的研究集中在识别几个声学特征，如基频（音调），话语强度[112]，带宽和持续时间。与独立于说话者的方法相比，与说话者相关的方法通常会提供更好的结果，如Navas等人的基准测试结果所示。 [113]，其中使用高斯混合模型（GMM）作为分类器实现了大约98％的准确度，其中韵律，语音质量以及MFCC用作语音特征。然而，在涉及大量用户的许多实际应用中，与说话者相关的方法是不可行的。

![Figure 27](https://github.com/THU-iar-AiLab/work_log/raw/master/images/27.png)

-----
# Local features vs. Global features

> *Audio affect classification is also classified into local features and global features. The common approach to analyze audio modality is to segment each audio/utterance into either overlapped or non-overlapped segments and examine them. Within a segment the signal is considered to be stationary. The features extracted from these segments are called local features.*

音频影响分类也分为局部特征和全局特征。 分析音频模态的常用方法是将每个音频/话语分段为重叠或非重叠片段并检查它们。 在一段中，信号被认为是静止的。 从这些段中提取的特征称为局部特征。

> *In speech production, there are several utterances and, for each utterance, the audio signal can be divided into several segments. Global features are calculated by measuring several statistics, e.g., average, mean, deviation of the local features. Global features are the most commonly used features in the literature. They are fast to compute and, as they are fewer in number compared to local features, the overall speed of computation is enhanced [114]. However, there are some drawbacks of calculating global features, as some of them are only useful to detect affect of high arousal, e.g., anger and disgust. For lower arousals, global features are not that effective, e.g., global features are less prominent to distinguish between anger and joy. Global features also lack temporal informa- tion and dependence between two segments in an utterance.*

在语音产生中，有几种话语，并且对于每种话语，音频信号可以分成几个段。通过测量若干统计数据来计算全局特征，例如，局部特征的平均值，平均值，偏差。全局特征是文献中最常用的特征。它们计算速度快，并且由于它们与局部特征相比数量更少，因此整体计算速度得到提高[114]。然而，计算全局特征存在一些缺点，因为它们中的一些仅用于检测高唤醒的影响，例如愤怒和厌恶。对于较低的觉醒，全局特征并不是那么有效，例如，全局特征在区分愤怒和喜悦方面不太突出。全局特征也缺乏话语中两个片段之间的时间信息和依赖性。

-----
# Speaker-independent applications

> *To the best of our knowledge, for speaker-independent applications, the best classification accuracy achieved to-date is 81% [115], obtained on the Berlin Database of Emotional Speech (BDES) [116] using a two-step classification approach and a unique set of spectral, prosodic, and voice features, selected through the Sequen- tial Floating Forward Selection (SFFS) algorithm [117]. As demonstrated in the analysis by Scherer et al. [118], human ability to recognize emotions from speech audio is about 60%. Their study showed that sadness and anger are detected more easily from speech, while the recognition of joy and fear is less reliable. Caridakis et al. [81] obtained 93.30% and 76.67% accuracy to identify anger and sadness, respectively, from speech, using 377 features based on intensity, pitch, MFCCs, Bark spectral bands, voiced segment characteristics, and pause length.*

据我们所知，对于与说话者无关的应用，到目前为止达到的最佳分类准确度为81％[115]，使用两步分类方法在柏林情绪语音数据库（BDES）[116]中获得。 通过序列浮动前向选择（SFFS）算法[117]选择的一组独特的频谱，韵律和语音特征。 正如Scherer等人的分析所证明的那样。 [118]，人类从语音中识别情绪的能力约为60％。 他们的研究表明，言语中更容易发现悲伤和愤怒，而对快乐和恐惧的认识则不那么可靠。 Caridakis等。 [81]使用基于强度，音高，MFCC，巴克光谱带，浊音段特征和暂停长度的377个特征，分别从语音中获得93.30％和76.67％的准确度以识别愤怒和悲伤。

-----
# Audio features extraction using deep networks


[95] C.-H. Wu, J.-F. Yeh, Z.-J. Chuang, Emotion perception and recognition from speech, in: Affective Information Processing, Springer, 2009, pp. 93–110.

[96] D. Morrison, R. Wang, L.C. De Silva, Ensemble methods for spoken emotion
recognition in call-centres, Speech Commun. 49 (2) (2007) 98–112.

[97] C.-H. Wu, W.-B. Liang, Emotion recognition of affective speech based on mul- tiple classifiers using acoustic-prosodic information and semantic labels, IEEE
Trans. Affective Comput. 2 (1) (2011) 10–21.

[98] I.R. Murray, J.L. Arnott, Toward the simulation of emotion in synthetic speech:
a review of the literature on human vocal emotion, J. Acoust. Soc. Am. 93 (2)
(1993) 1097–1108.

[99] I. Luengo, E. Navas, I. Hernáez, J. Sánchez, Automatic emotion recognition us-
ing prosodic parameters., in: Interspeech, 2005, pp. 493–496.

[100] S.G. Koolagudi, N. Kumar, K.S. Rao, Speech emotion recognition using seg- mental level prosodic analysis, in: Devices and Communications (ICDeCom),
2011 International Conference on, IEEE, 2011, pp. 1–5.

[101] D. Västfjäll, M. Kleiner, Emotion in product sound design, Proceedings of
Journées Design Sonore (2002).

[102] A. Batliner, K. Fischer, R. Huber, J. Spilker, E. Nöth, How to find trouble in
communication, Speech Commun. 40 (1) (2003) 117–143.

[103] C.M. Lee, S.S. Narayanan, Toward detecting emotions in spoken dialogs, IEEE
Trans.Speech Audio Process. 13 (2) (2005) 293–303.

[104] J. Hirschberg, S. Benus, J.M. Brenier, F. Enos, S. Friedman, S. Gilman, C. Girand,
M. Graciarena, A. Kathol, L. Michaelis, et al., Distinguishing deceptive from
non-deceptive speech., in: INTERSPEECH, 2005, pp. 1833–1836.

[105] L. Devillers, L. Vidrascu, L. Lamel, Challenges in real-life emotion annotation
and machine learning based detection, Neural Netw. 18 (4) (2005) 407–422.

[106] T. Vogt, E. André, Comparing feature sets for acted and spontaneous speech in view of automatic emotion recognition, in: Multimedia and Expo, 2005.
ICME 2005. IEEE International Conference on, IEEE, 2005, pp. 474–477.

[107] F. Eyben, M. Wöllmer, B. Schuller, Openear—introducing the munich open– source emotion and affect recognition toolkit, in: 2009 3rd International Con- ference on Affective Computing and Intelligent Interaction and Workshops,
IEEE, 2009, pp. 1–6.

[108] R.W. Levenson, Human emotion: a functional view, Nat.Emotion 1 (1994)
123–126.

[109] D. Datcu, L. Rothkrantz, Semantic audio-visual data fusion for automatic emo-
tion recognition, Euromedia’2008 (2008).

[110] F. Dellaert, T. Polzin, A. Waibel, Recognizing emotion in speech, in: Spoken
Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on,
3, IEEE, 1996, pp. 1970–1973.

[111] T. Johnstone, Emotional speech elicited using computer games, in: Spoken
Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on,
3, IEEE, 1996, pp. 1985–1988.

[112] L.S.-H. Chen, Joint Processing of Audio-Visual Information for the Recogni-
tion of Emotional Expressions in Human-Computer Interaction, Citeseer, 2000
Ph.D. thesis.

[113] E. Navas, I. Hernaez, I. Luengo, An objective and subjective study of the role of
semantics and prosodic features in building corpora for emotional tts, Audio
Speech Lang. Process. IEEE Transac. 14 (4) (2006) 1117–1127.

[114] M. El Ayadi, M.S. Kamel, F. Karray, Survey on speech emotion recogni- tion: features, classification schemes, and databases, Pattern Recognit. 44 (3)
(2011) 572–587.









-----
# 接下来的时间要全力冲刺论文啦，AAAI2019！:rocket:

题目——————吸引眼球

细化提纲—————逻辑连贯性

总分总结构

近两年欧美人书写风格

实验部分：实验设置、实验结果、讨论

三段式



特征工程

音频特征融合维度的深入讨论

不同分类不同维度特征融合的实验

-----
