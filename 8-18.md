# 接下来的时间要全力冲刺论文啦，AAAI2019！:rocket:

题目——————吸引眼球

细化提纲—————逻辑连贯性

总分总结构

近两年欧美人书写风格

实验部分：实验设置、实验结果、讨论

三段式



特征工程

音频特征融合维度的深入讨论

不同分类不同维度特征融合的实验

-----
# Audio modality

> *Similar to text and visual feature analysis, emotion and sentiment analysis through audio features has specific components. Several prosodic and acoustic features have been used in the literature to teach machines how to detect emotions [95–98]. Since emotional characteristics are more prominent in prosodic features, these features are widely used in the literature [99,100].*

与文本和视觉特征分析类似，通过音频特征进行的情绪和情感分析具有特定的组成部分。在文献中已经使用了几种韵律和声学特征来教授机器如何检测情绪[95-98]。由于情绪特征在韵律特征中更为突出，因此这些特征在文献中被广泛使用[99,100]。

> *Researchers started targeting affective reactions to everyday sounds [101], which have ultimately led to enormous applications to date, both in unimodal and multimodal analysis. The current trend is to understand affect in naturalistic videos [102–104], e.g., spontaneous dialogs, audio recordings collected in call centers, interviews, etc. Early research on extraction of audio features focused on the phonetic and acoustic properties of spoken language. With the help of psychological studies related to emotion, it was found that vocal parameters, especially pitch, intensity, speaking rate and voice quality play an important role in recognition of emotion and sentiment analysis [98].*

研究人员开始针对日常声音进行情感反应[101]，这最终导致了迄今为止在单模态和多模态分析中的巨大应用。目前的趋势是理解在自然的视频[102-104]中的影响，例如，自发的对话，在呼叫中心收集的录音，采访等。早期的音频特征提取研究主要集中在口语的语音和声学特性上。在与情绪相关的心理学研究的帮助下，发现声音参数，特别是音高，强度，说话率和语音质量在情绪和情感分析的识别中起着重要作用[98]。

> *Further studies showed that acoustic parameters change not only through oral variations, but are also dependent on personality traits. Various works have been carried out based on the types of features that are needed for better analysis [105,106]. Researchers have found pitch and energy related features playing a key role in affect recognition. Other features that have been used by some researchers for feature extraction include formants, mel frequency cepstral coefficients (MFCC), pause, teager energy operated based features, log frequency power coefficients (LFPC) and linear predic- tion cepstral coefficients (LPCC).*

进一步的研究表明，声学参数不仅通过口腔变化而改变，而且还取决于人格特质。为了更好的分析基于所需的特征类型已经进行了各种工作[105,106]。研究人员发现，音高和能量相关的特征在情感识别中起着关键作用。一些研究人员用于特征提取的其他特征包括共振峰，梅尔频率倒谱系数（MFCC），暂停，基于能量操作的特征，对数频率功率系数（LFPC）和线性预测倒谱系数（LPCC）。


[95] C.-H. Wu, J.-F. Yeh, Z.-J. Chuang, Emotion perception and recognition from speech, in: Affective Information Processing, Springer, 2009, pp. 93–110.

[96] D. Morrison, R. Wang, L.C. De Silva, Ensemble methods for spoken emotion
recognition in call-centres, Speech Commun. 49 (2) (2007) 98–112.

[97] C.-H. Wu, W.-B. Liang, Emotion recognition of affective speech based on mul- tiple classifiers using acoustic-prosodic information and semantic labels, IEEE
Trans. Affective Comput. 2 (1) (2011) 10–21.

[98] I.R. Murray, J.L. Arnott, Toward the simulation of emotion in synthetic speech:
a review of the literature on human vocal emotion, J. Acoust. Soc. Am. 93 (2)
(1993) 1097–1108.

[99] I. Luengo, E. Navas, I. Hernáez, J. Sánchez, Automatic emotion recognition us-
ing prosodic parameters., in: Interspeech, 2005, pp. 493–496.

[100] S.G. Koolagudi, N. Kumar, K.S. Rao, Speech emotion recognition using seg- mental level prosodic analysis, in: Devices and Communications (ICDeCom),
2011 International Conference on, IEEE, 2011, pp. 1–5.

[101] D. Västfjäll, M. Kleiner, Emotion in product sound design, Proceedings of
Journées Design Sonore (2002).

[102] A. Batliner, K. Fischer, R. Huber, J. Spilker, E. Nöth, How to find trouble in
communication, Speech Commun. 40 (1) (2003) 117–143.

[103] C.M. Lee, S.S. Narayanan, Toward detecting emotions in spoken dialogs, IEEE
Trans.Speech Audio Process. 13 (2) (2005) 293–303.

[104] J. Hirschberg, S. Benus, J.M. Brenier, F. Enos, S. Friedman, S. Gilman, C. Girand,
M. Graciarena, A. Kathol, L. Michaelis, et al., Distinguishing deceptive from
non-deceptive speech., in: INTERSPEECH, 2005, pp. 1833–1836.

[105] L. Devillers, L. Vidrascu, L. Lamel, Challenges in real-life emotion annotation
and machine learning based detection, Neural Netw. 18 (4) (2005) 407–422.

[106] T. Vogt, E. André, Comparing feature sets for acted and spontaneous speech in view of automatic emotion recognition, in: Multimedia and Expo, 2005.
ICME 2005. IEEE International Conference on, IEEE, 2005, pp. 474–477.


